{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data in Medical Imaging\n",
    "In medical imaging the data io is one of the most challenging steps. In this example we explore the different options for reading image and feeding them to Tensorflow.\n",
    "\n",
    "* Generators: use native python generator functions\n",
    "\n",
    "* Feeding: Python code provides the data when running each step\n",
    "\n",
    "* TFRecord: record-oriented file format to process large datasets that do not fit in memory\n",
    "\n",
    "Below you may find benchmarks for feed dictionaries, numpy generators and TFRecord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timer helper class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class Timer(object):\n",
    "    \"\"\"Timer class\n",
    "       Wrap a will with a timing function\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "    def __enter__(self):\n",
    "        self.t = time.time()\n",
    "        \n",
    "    def __exit__(self, *args, **kwargs):\n",
    "        print(\"{} took {} seconds\".format(\n",
    "        self.name, time.time() - self.t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import os\n",
    "from dltk.io.augmentation import *\n",
    "from dltk.io.preprocessing import *\n",
    "import tensorflow as tf\n",
    "\n",
    "# Use pandas to read csvs\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the batch size for all examples and number of iterations\n",
    "batch_size = 5\n",
    "iterations = 50\n",
    "\n",
    "all_filenames = pd.read_csv(\n",
    "    '../../data/IXI_HH/demographic_HH.csv',\n",
    "    dtype=object,\n",
    "    keep_default_na=False,\n",
    "    na_values=[]).as_matrix()\n",
    "\n",
    "# Keep the first 10 images\n",
    "all_filenames = all_filenames[:10]\n",
    "\n",
    "# Define the desired shapes of the examples and parameters to pass to `read_fn`:\n",
    "reader_params = {'n_examples': 1,\n",
    "                 'example_size': [128, 224, 224],\n",
    "                 'extract_examples': True}\n",
    "\n",
    "reader_example_shapes = {'features': {'x': reader_params['example_size'] + [1,]},\n",
    "                         'labels': {'y': []}}\n",
    "\n",
    "# Here, we would like to train our features and use labels as targets: \n",
    "reader_example_dtypes = {'features': {'x': tf.float32},\n",
    "                         'labels': {'y': tf.int32}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fn(file_references, mode, params=None):\n",
    "    \n",
    "    # We define a `read_fn` and iterate through the `file_references`, which\n",
    "    # can contain information about the data to be read (e.g. a file path):\n",
    "    for meta_data in file_references:\n",
    "        \n",
    "        # Here, we parse the `subject_id` to construct a file path to read\n",
    "        # an image from.\n",
    "        subject_id = meta_data[0]\n",
    "        data_path = '../../data/IXI_HH/1mm'\n",
    "        t1_fn = os.path.join(data_path, '{}/T1_1mm.nii.gz'.format(subject_id))\n",
    "        \n",
    "        # Read the .nii image containing a brain volume with SimpleITK and get \n",
    "        # the numpy array:\n",
    "        sitk_t1 = sitk.ReadImage(t1_fn)\n",
    "        t1 = sitk.GetArrayFromImage(sitk_t1)\n",
    "\n",
    "        # Normalise the image to zero mean/unit std dev:\n",
    "        t1 = whitening(t1)\n",
    "        \n",
    "        # Create a 4D Tensor with a dummy dimension for channels\n",
    "        t1 = t1[..., np.newaxis]\n",
    "        \n",
    "        # If in PREDICT mode, yield the image (because there will be no label\n",
    "        # present). Additionally, yield the sitk.Image pointer (including all\n",
    "        # the header information) and some metadata (e.g. the subject id),\n",
    "        # to facilitate post-processing (e.g. reslicing) and saving.\n",
    "        # This can be useful when you want to use the same read function as \n",
    "        # python generator for deployment.\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            yield {'features': {'x': t1}}\n",
    "\n",
    "        # Labels: Here, we parse the class *sex* from the file_references \n",
    "        # \\in [1,2] and shift them to \\in [0,1] for training:\n",
    "        sex = np.int32(meta_data[1]) - 1\n",
    "        y = sex\n",
    "        \n",
    "        # If training should be done on image patches for improved mixing, \n",
    "        # memory limitations or class balancing, call a patch extractor\n",
    "        if params['extract_examples']:\n",
    "            images = extract_random_example_array(\n",
    "                t1,\n",
    "                example_size=params['example_size'],\n",
    "                n_examples=params['n_examples'])\n",
    "            \n",
    "            # Loop the extracted image patches and yield\n",
    "            for e in range(params['n_examples']):\n",
    "                yield {'features': {'x': images[e].astype(np.float32)},\n",
    "                       'labels': {'y': y.astype(np.int32)}}\n",
    "                     \n",
    "        # If desired (i.e. for evaluation, etc.), return the full images\n",
    "        else:\n",
    "            yield {'features': {'x': images},\n",
    "                   'labels': {'y': y.astype(np.int32)}}\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "# Generator function\n",
    "def f():\n",
    "    fn = read_fn(file_references=all_filenames,\n",
    "                 mode=tf.estimator.ModeKeys.TRAIN, \n",
    "                 params=reader_params)\n",
    "    \n",
    "    ex = next(fn)\n",
    "    # Yield the next image\n",
    "    yield ex\n",
    "    \n",
    "with Timer('Generator'):\n",
    "    # Timed example with generator io\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        f, reader_example_dtypes, reader_example_shapes)\n",
    "    dataset = dataset.repeat(None)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(1)\n",
    "\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    next_dict = iterator.get_next()\n",
    "\n",
    "    with tf.train.MonitoredTrainingSession() as sess_gen:\n",
    "        # Initialize generator\n",
    "        sess_gen.run(iterator.initializer)\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            # Fetch the next batch of images\n",
    "            gen_batch_feat, gen_batch_lbl = sess_gen.run([next_dict['features'], next_dict['labels']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the `gen_batch_feat` using matplotlib.\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(gen_batch_feat['x'][3, 0, :, :, 0], 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using feed dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_references, mode, params=None):\n",
    "    \n",
    "    data = {'features': [], 'labels': []}\n",
    "    \n",
    "    # We define a `read_fn` and iterate through the `file_references`, which\n",
    "    # can contain information about the data to be read (e.g. a file path):\n",
    "    for meta_data in file_references:\n",
    "        \n",
    "        # Here, we parse the `subject_id` to construct a file path to read\n",
    "        # an image from.\n",
    "        subject_id = meta_data[0]\n",
    "        data_path = '../../data/IXI_HH/1mm'\n",
    "        t1_fn = os.path.join(data_path, '{}/T1_1mm.nii.gz'.format(subject_id))\n",
    "        \n",
    "        # Read the .nii image containing a brain volume with SimpleITK and get \n",
    "        # the numpy array:\n",
    "        sitk_t1 = sitk.ReadImage(t1_fn)\n",
    "        t1 = sitk.GetArrayFromImage(sitk_t1)\n",
    "\n",
    "        # Normalise the image to zero mean/unit std dev:\n",
    "        t1 = whitening(t1)\n",
    "        \n",
    "        # Create a 4D Tensor with a dummy dimension for channels\n",
    "        t1 = t1[..., np.newaxis]\n",
    "\n",
    "        # Labels: Here, we parse the class *sex* from the file_references \n",
    "        # \\in [1,2] and shift them to \\in [0,1] for training:\n",
    "        sex = np.int32(meta_data[1]) - 1\n",
    "        y = sex\n",
    "        \n",
    "        # If training should be done on image patches for improved mixing, \n",
    "        # memory limitations or class balancing, call a patch extractor\n",
    "        if params['extract_examples']:\n",
    "            images = extract_random_example_array(\n",
    "                t1,\n",
    "                example_size=params['example_size'],\n",
    "                n_examples=params['n_examples'])\n",
    "            \n",
    "            # Loop the extracted image patches\n",
    "            for e in range(params['n_examples']):\n",
    "                data['features'].append(images[e].astype(np.float32))\n",
    "                data['labels'].append(y.astype(np.int32))\n",
    "                     \n",
    "        # If desired (i.e. for evaluation, etc.), return the full images\n",
    "        else:\n",
    "            data['features'].append(images)\n",
    "            data['labels'].append(y.astype(np.int32))\n",
    "\n",
    "    data['features'] = np.array(data['features'])\n",
    "    data['labels'] = np.vstack(data['labels'])\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timed feed dictionary example\n",
    "data = load_data(all_filenames, \n",
    "                 tf.estimator.ModeKeys.TRAIN, reader_params)\n",
    "\n",
    "with Timer('Feed dictionary'):\n",
    "    x = tf.placeholder(reader_example_dtypes['features']['x'], \n",
    "                       [None, 128, 224, 224, 1])\n",
    "    y = tf.placeholder(reader_example_dtypes['labels']['y'], \n",
    "                       [None, 1])\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    dataset = dataset.repeat(None)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(1)\n",
    "\n",
    "    features = data['features']\n",
    "    labels = data['labels']\n",
    "    # Check that features and labels dimensions match\n",
    "    assert features.shape[0] == labels.shape[0]\n",
    "\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    nx = iterator.get_next()\n",
    "\n",
    "    with tf.train.MonitoredTrainingSession() as sess_dict:\n",
    "        # Initialize iterator\n",
    "        sess_dict.run(iterator.initializer, \n",
    "                   feed_dict={x: features, y: labels})\n",
    "        for i in range(iterations):\n",
    "            # Get next features-labels pair\n",
    "            dict_batch_feat, dict_batch_lbl = sess_dict.run(nx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the `dict_batch_feat` using matplotlib.\n",
    "plt.imshow(dict_batch_feat[3, 0, :, :, 0], 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TFRecord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data for a single subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(meta_data, params):\n",
    "    x = []\n",
    "    \n",
    "    # Here, we parse the `subject_id` to construct a file path to read\n",
    "    # an image from.\n",
    "    subject_id = meta_data[0]\n",
    "    data_path = '../../data/IXI_HH/1mm'\n",
    "    t1_fn = os.path.join(data_path, '{}/T1_1mm.nii.gz'.format(subject_id))\n",
    "\n",
    "    # Read the .nii image containing a brain volume with SimpleITK and get \n",
    "    # the numpy array:\n",
    "    sitk_t1 = sitk.ReadImage(t1_fn)\n",
    "    t1 = sitk.GetArrayFromImage(sitk_t1)\n",
    "\n",
    "    # Normalise the image to zero mean/unit std dev:\n",
    "    t1 = whitening(t1)\n",
    "    \n",
    "    # Create a 4D Tensor with a dummy dimension for channels\n",
    "    t1 = t1[..., np.newaxis]\n",
    "\n",
    "    # Labels: Here, we parse the class *sex* from the file_references \n",
    "    # \\in [1,2] and shift them to \\in [0,1] for training:\n",
    "    sex = np.int32(meta_data[1]) - 1\n",
    "    y = sex\n",
    "    \n",
    "    # If training should be done on image patches for improved mixing, \n",
    "    # memory limitations or class balancing, call a patch extractor\n",
    "    if params['extract_examples']:\n",
    "        images = extract_random_example_array(\n",
    "            t1,\n",
    "            example_size=params['example_size'],\n",
    "            n_examples=params['n_examples'])\n",
    "\n",
    "        # Loop the extracted image patches and yield\n",
    "        for e in range(params['n_examples']):\n",
    "            x.append(images[e].astype(np.float32))\n",
    "\n",
    "    # If desired (i.e. for evaluation, etc.), return the full images\n",
    "    else:\n",
    "        x = images\n",
    "            \n",
    "    return np.array(x), y  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert data to features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write data into a TFRecords file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = 'train.tfrecords'  # address to save the TFRecords file\n",
    "# open the TFRecords file\n",
    "writer = tf.python_io.TFRecordWriter(train_filename)\n",
    "\n",
    "for meta_data in all_filenames:\n",
    "    # Load the image\n",
    "    img, label = load_img(meta_data, reader_params)\n",
    "    # Create a feature\n",
    "    feature = {'train/label': _int64_feature(label),\n",
    "               'train/image': _float_feature(img.ravel())}\n",
    "    # Create an example protocol buffer\n",
    "    example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    \n",
    "    # Serialize to string and write on the file\n",
    "    writer.write(example.SerializeToString())\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load TFRecord and decode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(serialized_example):\n",
    "    # NOTE: You might get an error here, because it seems unlikely that the features\n",
    "    # called 'coord2d' and 'coord3d', and produced using `ndarray.flatten()`, will\n",
    "    # have a scalar shape. You might need to change the shape passed to\n",
    "    # `tf.FixedLenFeature()`.\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={'train/image': tf.FixedLenFeature([128, 224, 224, 1], tf.float32),\n",
    "                  'train/label': tf.FixedLenFeature([], tf.int64)})\n",
    "\n",
    "    # NOTE: No need to cast these features, as they are already `tf.float32` values.\n",
    "    return features['train/image'], features['train/label']\n",
    "\n",
    "\n",
    "with Timer('TFRecord'):\n",
    "    dataset = tf.data.TFRecordDataset(train_filename).map(decode)\n",
    "    dataset = dataset.repeat(None)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(1)\n",
    "    \n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    features, labels = iterator.get_next()\n",
    "    nx = iterator.get_next()\n",
    "\n",
    "    with tf.train.MonitoredTrainingSession() as sess_rec:\n",
    "        sess_rec.run(iterator.initializer)\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            try:\n",
    "                # Get next features-labels pair\n",
    "                rec_batch_feat, rec_batch_lbl = sess_rec.run([features, labels])\n",
    "                \n",
    "            except tf.errors.OutOfRangeError:\n",
    "                # Raised when we reach the end of the file.\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the `rec_batch_feat` using matplotlib.\n",
    "plt.imshow(rec_batch_feat[3, 0, :, :, 0], 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dltk_venv3",
   "language": "python",
   "name": "dltk_venv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
